{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ROC Curves\n",
    "\n",
    "---\n",
    "\n",
    "Classification methods can do more than output labels, including outputting probabilities. Imagine a classifier that comes up with scores for ten individuals who might have a disease. The scores range from 0.0 to 1.0. .7 is determined to be the best break point to distinguish between those with the disease and those without. \n",
    "values = np.arange(0.05, 1, .1)\n",
    "y_true = [0, 0, 0, 1, 1, 0, 0, 1, 1, 1]\n",
    "y_pred = [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]\n",
    "\n",
    "df = pd.DataFrame({'Values': values,\n",
    "                   'y_true': y_true,\n",
    "                   'y_pred': y_pred})\n",
    "\n",
    "display(df)\n",
    "\n",
    "d = {'Pred Negative': ['.05 .15 .25', '.35 .45'],\n",
    "     'Pred Positive': ['.55 .65', '.75 .85 .95']}\n",
    "\n",
    "df = pd.DataFrame(data=d, index=['Real Negative', 'Real Positive'])\n",
    "display(df)\n",
    "\n",
    "1. How do we evaluate and select this threshold? How do we pick a tradeoff value between false positives and false negatives?\n",
    "2. How do we compare two different classification systems, both of which have a whole range of possible tradeoffs?\n",
    "\n",
    "The solution is to use the ROC curve, the _Receiver Operating Characteristic_ curve, which has a long history in classification and was initially used in World War II to quantify radar tracking of bombers headed towards England. ROC curves are drawn in terms of the _sensitivity_, otherwise known as the _true positive rate_ or TPR, compared to the _false positive rate_ or FPR, which is 1 - the _specificity_. Both of these metrics measure performance in the real world. We want the classifier to have a _high_ TPR, 1.0 being the best score, and a _low__ FPR, 0.0 being the best score. \n",
    "### Binary ROC\n",
    "\n",
    "---\n",
    "\n",
    "In `sklearn`, we can use the `roc_curve` function, which computes Receiver operating characteristic curves and is restricted to the binary classification task. The function accepts true binary labels and target scores, which can either be probability estimates of the positive class, confidence values, or non-thresholded measures of decisions (as returned by the `decision_function` on some classifiers. \n",
    "\n",
    "How do we make ROC work? There's a single call, `roc_curve` , in the `sklearn.metrics` module that does the heavy lifting after a couple of setup steps. First, let's convert the iris problem into a *binary* classification task to simplify the interpretation of the results. The binary question asks, \"Is it *Versicolor*?\" The answer is yes or no. Also, we need to invoke the classification scoring mechanism of our classifier so we can tell who is on which side of our prediction bar. Instead of outputting a class like *Versicolor*, we need to know a probability, such as a .7 likelihood of *Versicolor*. We do this by using `predict_proba` instead of the typical `predict` method. `predict_proba` returns probabilities for *False* and *True* in two columns. We are interested in the probability from the *True* column for building the ROC curve.\n",
    "\n",
    "\n",
    "is_versicolor = iris.target == 1\n",
    "\n",
    "tts_1c = train_test_split(iris.data, is_versicolor,\n",
    "                          test_size=.33, random_state=21)\n",
    "\n",
    "(iris_1c_train_ftrs, iris_1c_test_ftrs,\n",
    " iris_1c_train_tgt, iris_1c_test_tgt) = tts_1c\n",
    "# build, fit, predict (probability scores) for NB model\n",
    "gnb = GaussianNB()\n",
    "prob_true = (gnb.fit(iris_1c_train_ftrs, iris_1c_train_tgt)\n",
    "             .predict_proba(iris_1c_test_ftrs)[:, 1])  # [:, 1]==\"True\"\n",
    "\n",
    "With the setup done, we can do the calculations for the ROC curve and display it.\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresh = roc_curve(iris_1c_test_tgt, prob_true)\n",
    "auc = auc(fpr, tpr)\n",
    "\n",
    "# create the main graph\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "ax.plot(fpr, tpr, 'o--')\n",
    "ax.set_title(f\"1-Class Iris ROC Curve\\nAUC:{auc:.3f}\")\n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylabel(\"TPR\")\n",
    "\n",
    "# do a bit of work to label some points with their respective thresholds\n",
    "investigate = np.array([1, 3, 5])\n",
    "for idx in investigate:\n",
    "    th, f, t = thresh[idx], fpr[idx], tpr[idx]\n",
    "    ax.annotate(f'thresh = {th:.3f}', xy=(f+.01, t-.01),\n",
    "                xytext=(f+.1, t), arrowprops={'arrowstyle': '->', 'color': 'black'}, size=17)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "Most of the FPR values are between 0.0 and 0.2, while the TPR values quickly jump into the range of 0.9 to 1.0. Let's take a look at the calculation of those values. Each point represents a different confusion matrix based on its own unique threshold. The following shows the confusion matrices for the second, fourth, and sixth thresholds labeled in the last graph. Due to zero-based ine to the variable xing, these occur at indices 1, 3, 5and 5 which were assigned to the variable `investigate` in the previous cell. We could have picked any of the eight thresholds that `sklearn` found. Let's look at these values.\n",
    "title_fmt = \"Threshold {}\\n~{:5.3f}\\nTPR : {:.3f}\\nFPR : {:.3f}\"\n",
    "\n",
    "np = ['Negative', 'Positive']\n",
    "add_args = {'xticklabels': np,\n",
    "            'yticklabels': np,\n",
    "            'square': True}\n",
    "fig, axes = plt.subplots(1, 3, sharey=True, figsize=(12, 8))\n",
    "for ax, thresh_idx in zip(axes.flat, investigate):\n",
    "    preds_at_th = prob_true >= thresh[thresh_idx]\n",
    "    cm = confusion_matrix(iris_1c_test_tgt, preds_at_th)\n",
    "    sns.heatmap(cm, annot=True, cbar=False, ax=ax, cmap='flare', **add_args)\n",
    "\n",
    "    ax.set_xlabel('Predicted')\n",
    "    ax.set_title(title_fmt.format(thresh_idx,\n",
    "                                  thresh[thresh_idx],\n",
    "                                  tpr[thresh_idx],\n",
    "                                  fpr[thresh_idx]))\n",
    "\n",
    "axes[0].set_ylabel('Actual')\n",
    "plt.show()\n",
    "\n",
    "Let's say we want use the confusion matrix to determine how well a classifier identifies sick people.\n",
    "\n",
    "### AUC: Area-Under-the-(ROC)-Curve\n",
    "\n",
    "---\n",
    "\n",
    "How can we summarize an ROC curve as a single value? We answer by calculating the *area under the curve* (AUC) that we've just drawn. The AUC is an *overall* measure of classifier performance at a series of thresholds. The benefit of single-value summaries is that we can easily compute other statistics on them and summarize them graphically. Let's look at several cross-validated AUCs displayed simultaneously on a strip plot.\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(3, 3))\n",
    "model = sklearn.neighbors.KNeighborsClassifier(3)\n",
    "cv_auc = sklearn.model_selection.cross_val_score(\n",
    "    model, iris.data, iris.target == 1, scoring='roc_auc', cv=10)\n",
    "ax = sns.swarmplot(cv_auc, orient='v')\n",
    "ax.set_title('10-Fold AUCs')\n",
    "\n",
    "Many folds return perfect results.\n",
    "\n",
    "`sklearn.metrics.roc_curve` is ill-equipped to deal with multiclass problems. We can work around this by recoding our tri-class problem into a series of me-versus-the-world or one-versus-rest (OvR) alternatives. OvR means we compare each of the following binary problems: 0 versus [1, 2], 1 versus [0, 2]; , and 2 versus [0, 2]. The difference here is that we do it for all three possibilities. The basic tool to encode these comparisons into our data is `label_binarize` . Let's look at examples 0, 50, and 100 from the original multiclass data.\n",
    "\n",
    "## Multiclass Learners, One-versus-Rest, and ROC\n",
    "\n",
    "---\n",
    "\n",
    "`sklearn.metrics.roc_curve` is ill-equipped to deal with multiclass problems. We can work around this by recoding our tri-class problem into a series of me-versus-the-world or one-versus-rest (OvR) alternatives. OvR means we compare each of the following binary problems: 0 versus [1, 2], 1 versus [0, 2]; , and 2 versus [0, 2]. The difference here is that we do it for all three possibilities. The basic tool to encode these comparisons into our data is `label_binarize` . Let's look at examples 0, 50, and 100 from the original multiclass data.\n",
    "\n",
    "Therefore, examples 0, 50, and 100 correspond to classes 0, 1, and 2. When we binarize, the classes become:\n",
    "\n",
    "print(\"'Multi-label' Encoding\")\n",
    "print(sklearn.preprocessing.label_binarize(\n",
    "    y=iris.target, classes=[0, 1, 2])[checkout])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "d = {'0': ['1', '0', '0'],\n",
    "     '1': ['0', '1', '0'],\n",
    "     '2': ['0', '0', '1']}\n",
    "\n",
    "df = pd.DataFrame(data=d, index=['0', '1', '2'])\n",
    "df\n",
    "\n",
    "Let's look at another example.\n",
    "\n",
    "from sklearn.preprocessing import label_binarize\n",
    "label_binarize(y=[1, 6], classes=[1, 2, 4, 6])\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "d = {'1': ['1', '0'],\n",
    "     '2': ['0', '0'],\n",
    "     '4': ['0', '0'],\n",
    "     '6': ['0', '1']}\n",
    "\n",
    "df = pd.DataFrame(data=d, index=['1', '6'])\n",
    "df\n",
    "\n",
    "The class ordering is preserved:\n",
    "\n",
    "label_binarize([1, 6], classes=[1, 6, 4, 2])\n",
    "\n",
    "Binary targets transfrom to a column vector:\n",
    "\n",
    "sklearn.preprocessing.label_binarize(\n",
    "    ['yes', 'no', 'no', 'yes'], classes=['no', 'yes'])\n",
    "\n",
    "These encodings are columns of Boolean flags--yes/no for \"Is it class _x_?\". The first column answers, \"Is it class 0?\" and the answers are yes, no, and no. Now, we add a layer of complexity to our classifier. Instead of a *single* classifier, we are going to make one classifier for each target class that was just added, the three new target columns. These become (1) a classifier for class 0 versus the rest, (2) a classifier for class 1 versus the rest, and (3) a classifier for class 2 versus the rest. Then, we can look at the individual performance of the three classifiers.\n",
    "\n",
    "iris_multi_tgt = sklearn.preprocessing.label_binarize(\n",
    "    y=iris.target, classes=[0, 1, 2])\n",
    "\n",
    "# im --> \"iris multi\"\n",
    "\n",
    "(im_train_ftrs, im_test_ftrs,\n",
    " im_train_tgt, im_test_tgt) = sklearn.model_selection.train_test_split(iris.data,\n",
    "                                                                       iris_multi_tgt, test_size=.33, random_state=21)\n",
    "\n",
    "# knn wrapped up in one-versus-rest (3 classifiers)\n",
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "ovr_knn = sklearn.multiclass.OneVsRestClassifier(knn)\n",
    "pred_probs = (ovr_knn.fit(im_train_ftrs, im_train_tgt)\n",
    "              .predict_proba(im_test_ftrs))\n",
    "\n",
    "# make ROC plots\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "for cls in [0, 1, 2]:\n",
    "    fpr, tpr, _ = sklearn.metrics.roc_curve(im_test_tgt[:, cls],\n",
    "                                            pred_probs[:, cls])\n",
    "\n",
    "    label = f'Class {cls} vs Rest (AUC = {sklearn.metrics.auc(fpr,tpr):.2f})'\n",
    "    ax.plot(fpr, tpr, 'o--', label=label)\n",
    "\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"FPR\")\n",
    "ax.set_ylabel(\"TPR\")\n",
    "\n",
    "# Another Take on Multiclass: One-versus-One\n",
    "\n",
    "There is another take on dealing with the sometimes negative interaction between multiclass problems and learning systems. In one-versus-rest, we chunk off apples against all other fruit in one grand binary problem. For apples, we create *one* one-versus-rest classifier. Another way to do this is to chuch off apple-versus-banana, apple-versus-orange, and so on. Then, instead of one grand Boolean comparison for apples, we make $n - 1$ comparisons, where  $n$ is the number of classes we have. This alternative is called *one-versus-one*. How do we wrap the one-versus-one winners into a grand winner for making a single prediction? We can take the sums of the individual wins and the class with the biggest number of wins in the class we predict. The one-versus-one wrapper gives us *classification scores* for each individual class. The values are *not* probabilities. We take the index of the maximum classification score to find the single best-predicted class.\n",
    "\n",
    "knn = sklearn.neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "ovo_knn = sklearn.multiclass.OneVsOneClassifier(knn)\n",
    "pred_scores = (ovo_knn.fit(iris_train_ftrs, iris_train_tgt)\n",
    "                      .decision_function(iris_test_ftrs))\n",
    "df = pd.DataFrame(pred_scores)\n",
    "df['class'] = df.values.argmax(axis=1)\n",
    "display(df.head())\n",
    "\n",
    "Let's put the actual classes beside the one-versus-one classification scores:\n",
    "\n",
    "# note: ugly way to make column headers\n",
    "mi = pd.MultiIndex([['Class Indicator', 'Vote'], [0, 1, 2]],\n",
    "                   [[0]*3+[1]*3, list(range(3)) * 2])\n",
    "df = pd.DataFrame(np.c_[im_test_tgt, pred_scores], columns=mi)\n",
    "display(df.head())\n",
    "\n",
    "Parameters_index = '\\n'.join(\n",
    "    pd.MultiIndex.__doc__.splitlines()).index('Parameters')\n",
    "print('\\n'.join(pd.MultiIndex.__doc__.splitlines())[:])\n",
    "\n",
    "### DataFrames\n",
    "\n",
    "There are multiple ways to construct a DataFrame. For instance we can use a dictionary.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "d = {'Age': ['25', '26'],\n",
    "     'Name': ['Scott', 'Linda']}\n",
    "\n",
    "df = pd.DataFrame(data=d)\n",
    "df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "d = {'Name': ['Scott', 'Linda'],\n",
    "     'Age': ['25', '26']}\n",
    "\n",
    "df = pd.DataFrame(data=d, index=['Subject1', 'Subject2'])\n",
    "df.index.name = 'Person'\n",
    "df\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data = {'Medical Claim': ['Easy to call sick', 'Hard to call sick'],\n",
    "        'Prediction': ['Easy to predict True', 'Hard to predict False']}\n",
    "\n",
    "df = pd.DataFrame(data=data, index=['Low', 'High'])\n",
    "df.index.name = 'Bar'\n",
    "df\n",
    "\n",
    "Examples_index = '\\n'.join(pd.DataFrame.__doc__.splitlines()).index('Examples')\n",
    "print('\\n'.join(pd.DataFrame.__doc__.splitlines())[Examples_index:])\n",
    "\n",
    " \n",
    "\n",
    "Each row in a confusion matrix represents an actual class, while each column represents a predicted class.\n",
    "\n",
    "In the above matrix, the number in the first row and first column indicates that the classifier correctly identified 53, 124 images as non-5s. These are also known as *true negatives*. The remaining 1, 455 images in the first row and second column are known as *false positives* and represent images the classifier incorrectly categorized as 5s, which indeed were not.\n",
    "\n",
    "Each row in a confusion matrix represents an *actual class*, while each column represents a *predicted class*. \n",
    "In the above matrix, the number in the first row and first column indicates that the classifier correctly identified 53, 124 images as non-5s. These are also known as *true negatives*. The remaining 1, 455 images in the first row and second column are known as *false positives* and represent images the classifier incorrectly categorized as 5s, which indeed were not. \n",
    "\n",
    "Next, in the second row, the number in the first column (949) indicates the images that the classifier incorrectly identified as non-5s. In other words, the images were fives, but the classifier said they were not. These are known as *false negatives*. Finally, the remaining 4, 472 images in the bottom right corner are *true positives*, the fives that the classifier identified correctly. A perfect classifier would have only true positives and true negatives.\n",
    "\n",
    "Using a confusion matrix, we can compute additional metrics, one of which is the *precision* or the accuracy of the positive predictions. Let's take a look at a few more simple examples.\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "y_true = [2, 0, 2, 2, 0, 1]\n",
    "y_pred = [0, 0, 2, 2, 0, 2]\n",
    "confusion_matrix(y_true, y_pred)\n",
    "\n",
    "y_true = [\"cat\", \"ant\", \"cat\", \"cat\", \"ant\", \"bird\"]\n",
    "y_pred = [\"ant\", \"ant\", \"cat\", \"cat\", \"ant\", \"cat\"]\n",
    "confusion_matrix(y_true, y_pred, labels=[\"ant\", \"bird\", \"cat\"])\n",
    "\n",
    "In the case of a binary classifier, we can extract the true positives, false positives, true negatives, and false negatives.\n",
    "\n",
    "Not all these metrics are designed for classifiers. How can we identify the scorer used for a particular classifier, say, *k*-NN? You can see the whole output with `help(knn.score).` However, let's trim it down a bit.\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# help(knn.score) # verbose, but complete\n",
    "\n",
    "# print(knn.score.__doc__.splitlines()[0])\n",
    "# print('\\n--and--\\n')\n",
    "examples_index = \"\\n\".join(np.ravel.__doc__.splitlines()).index('Examples')\n",
    "print(\"\\n\".join(np.ravel.__doc__.splitlines())[examples_index:])\n",
    "search_word_index = '\\n'.join(\n",
    "    np.ravel.__doc__.splitlines()).index('search_word')\n",
    "print('\\n'.join(np.ravel.__doc__.splitlines())[search_word_index:])\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix([0, 1, 0, 1], [1, 1, 1, 0]).ravel()\n",
    "(tn, fp, fn, tp)\n",
    "\n",
    "Using the 5 and non-5 classifier, we get the following results.\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_train_5, y_train_pred).ravel()\n",
    "\n",
    "## `ravel()`\n",
    "\n",
    "---\n",
    "(tn, fp, fn, tp)\n",
    "\n",
    "Using the 5 and non-5 classifier, we get the following results.\n",
    "\n",
    "confusion_matrix(y_train_5, y_train_pred)\n",
    "\n",
    "We can use the $\\text{TN}$, $\\text{FP}$, $\\text{FN}$, and $\\text{TP}$ to calculate additional metrics. For instance, **precison** represents the accuracy of the positive predictions and is calculated via the following equation:\n",
    "\n",
    "$$ \\text{precision} = \\frac{TP}{TP + FP} $$\n",
    "\n",
    "Let's calculate the precision.\n",
    "\n",
    "precision = tp / (tp + fp)\n",
    "print(f'Precision: {precision:.2f}')\n",
    "\n",
    " `numpy.ravel()`\n",
    "\n",
    "# average is a weighted macro average\n",
    "\n",
    "# verify sums-across-rows\n",
    "cm = confusion_matrix(iris_test_tgt, tgt_preds)\n",
    "print(\"row counts equal support:\", cm.sum(axis=1))\n",
    "# define reality\n",
    "act_pos = [1 for _ in range(100)]\n",
    "act_neg = [0 for _ in range(10000)]\n",
    "y_true = act_pos + act_neg\n",
    "# define predictions\n",
    "pred_pos = [0 for _ in range(5)] + [1 for _ in range(95)]\n",
    "pred_neg = [1 for _ in range(55)] + [0 for _ in range(9945)]\n",
    "y_pred = pred_pos + pred_neg\n",
    "# calculate score\n",
    "score = f1_score(y_true, y_pred, average='binary')\n",
    "print(f'F-Measure: {score:.3f}')\n",
    "Let's confirm this calculation.\n",
    "# Receiver Operating Characteristic (ROC) with cross-validation\n",
    "\n",
    "---\n",
    "\n",
    "The ROC metric can be used to evaluate classifier output quality. The ROC curve typically features the true positive rate on the Y-axis and the false positive rate on the X-axis. The top left corner of the point is the \"ideal\" point, representing a false positive rate of zero and a true positive rate of one. This result is often unrealistic, but a larger area under the curve is usually better. Also, the \"steepness\" of the ROC curve is important since it is ideal to maximize the true positive rate while minimizing the false positive rate.\n",
    "\n",
    "The following shows the ROC response of different datasets created from K-fold cross-validation. Taking all of these cuves we can calculates the mean area under the curve and see the variance of the curve when the training set is split into different subsets, which shows how the classifier output is affected by changes in the training dta, and how different the splits generated by *k*-fold cross-validation afre from one a\n",
    "\n",
    "# Data IO and generation\n",
    "\n",
    "---\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "\n",
    "# Import some data to play with\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X, y = X[y != 2], y[y != 2]\n",
    "n_samples, n_features = X.shape\n",
    "\n",
    "# Add noisy features\n",
    "random_state = np.random.RandomState(0)\n",
    "X = np.c_[X, random_state.randn(n_samples, 200 * n_features)]\n",
    "X\n",
    "\n",
    "## Classification and ROC analysis\n",
    "\n",
    "---\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Run classifier with cross-validation and plot ROC curves\n",
    "cv = StratifiedKFold(n_splits=6)\n",
    "classifier = svm.SVC(kernel=\"linear\", probability=True,\n",
    "                     random_state=random_state)\n",
    "\n",
    "tprs = []\n",
    "aucs = []\n",
    "mean_fpr = np.linspace(0, 1, 100)\n",
    "fig, ax = plt.subplots()\n",
    "for i, (train, test) in enumerate(cv.split(X, y)):\n",
    "    classifier.fit(X[train], y[train])\n",
    "    viz = RocCurveDisplay.from_estimator(\n",
    "        classifier,\n",
    "        X[test],\n",
    "        y[test],\n",
    "        name=\"ROC fold {}\".format(i),\n",
    "        alpha=0.3,\n",
    "        lw=1,\n",
    "        ax=ax,\n",
    "    )\n",
    "    interp_tpr = np.interp(mean_fpr, viz.fpr, viz.tpr)\n",
    "    interp_tpr[0] = 0.0\n",
    "    tprs.append(interp_tpr)\n",
    "    aucs.append(viz.roc_auc)\n",
    "\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2,\n",
    "        color=\"r\", label=\"Chance\", alpha=0.8)\n",
    "\n",
    "mean_tpr = np.mean(tprs, axis=0)\n",
    "mean_tpr[-1] = 1.0\n",
    "mean_auc = auc(mean_fpr, mean_tpr)\n",
    "std_auc = np.std(aucs)\n",
    "ax.plot(\n",
    "    mean_fpr,\n",
    "    mean_tpr,\n",
    "    color=\"b\",\n",
    "    label=r\"Mean ROC (AUC = %0.2f $\\pm$ %0.2f)\" % (mean_auc, std_auc),\n",
    "    lw=2,\n",
    "    alpha=0.8,\n",
    ")\n",
    "\n",
    "std_tpr = np.std(tprs, axis=0)\n",
    "tprs_upper = np.minimum(mean_tpr + std_tpr, 1)\n",
    "tprs_lower = np.maximum(mean_tpr - std_tpr, 0)\n",
    "ax.fill_between(\n",
    "    mean_fpr,\n",
    "    tprs_lower,\n",
    "    tprs_upper,\n",
    "    color=\"grey\",\n",
    "    alpha=0.2,\n",
    "    label=r\"$\\pm$ 1 std. dev.\",\n",
    ")\n",
    "\n",
    "ax.set(\n",
    "    xlim=[-0.05, 1.05],\n",
    "    ylim=[-0.05, 1.05],\n",
    "    title=\"Receiver operating characteristic example\",\n",
    ")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "How do we compress the rich information in a many-values confusion matrix into simpler values?\n",
    "\n",
    "In the classification above, we made three mistakes, predicting one Versicolor as Virginica and two Virginica as Versicolor. In our two-class metrics, it was the precision that drew out information about the positive prediction column. When we predict Versicolor, we are correct 16 times and wrong 2. Considering Versicolor by itself, we can calculate something similar to precision giving us $$\\frac{16}{18} \\approx .89 $$\n",
    "Similarly, for the Virginica class we get the following. $$\\frac{13}{14} \\approx .93 $$\n",
    "Finally, for the Setosa class, we predict $$\\frac{18}{18} = 1.0 $$\n",
    "The mean of $\\{\\frac{16}{18}, \\frac{13}{14}, 1\\}$ is about .9392.\n",
    "\n",
    "from fractions import Fraction\n",
    "import numpy as np\n",
    "Versicolor_acc = 16/18\n",
    "Virginica_acc = 13/14\n",
    "Setosa_acc = 1\n",
    "accuracies = [Versicolor_acc, Virginica_acc, Setosa_acc]\n",
    "accuracies_arr = np.array(accuracies)\n",
    "avg = np.average(accuracies_arr)\n",
    "round(avg, 4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.0 64-bit ('3.9.0')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3fe900b466f292f4cbc35e05862ed7935ccda66699503d09b7e70e5c7d339eb0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
